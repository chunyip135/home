[{"categories":null,"contents":"Addressed pretty significant page load performance issue founde in larger deployments. Eliminates uses of intensive backend query, replacing it with an asynchronous API call against a lucene index. This change reduces page load from from 2+ minutes to nearly instant, with an incredibly responsive UI.\n","permalink":"https://chunyip135.github.io/home/projects/contributions/deploy-triggers/","tags":["Java","jQuery","REST APIs","Bamboo","JSON"],"title":"Atlassian Deployment Triggers"},{"categories":null,"contents":"My pathway in learning Data Science  note that :  1st phase =\u0026gt; earlier, beginner stage, all task completed 2nd phase =\u0026gt; current stage, many incomplete task 3rd phase =\u0026gt; future path, things to learn in future    1st phase Courses  Data Science using R career tracks on DataCamp  R programming dplyr library     tidyverse library  ggplot used my knowledge to complete one of my Probability and Statistic\u0026rsquo;s assignment    DataCamp\u0026rsquo;s courses   Shell Introduction to SQL  Data Science using Python career tracks on DataCamp   Python programming  functions   More complete data science skills Importing data using Pandas Data Cleaning Data Wrangling EDA Data Visualization using pandas, matplotlib, seaborn, missingno Interactive Data Viz using Bokeh Statistical thinking Time-series Supervised \u0026amp; unsupervised tree based model like DT, RF \u0026hellip; linear models like lr, logreg web scraping cluser analysis OOP Geospatial data  Projects  Titanic Loan prediction Big mart sales  2nd phase Courses   Version control\n Git Github    Hands-on ML 2 book\n Various ML algorithmns more on theory and hyperparameters PCA    Data Vizualisation\n Plotly Altair Streamlit Tableau using elearning    Python for Data Analysis book\n  Deep Learning Specialization on Coursera\n Neural networks  Linear algebra Neural networks (shallow \u0026amp; DNN) backpropagation \u0026amp; forward-propagation perceptron      Tensorflow specialization\n Introduction to Tensorflow for AI, ML and DL  keras api model CNN computer vision using ImageDataGenerator   Convolutional Neural Network in Tensorflow  data augmentation transfer learning multi-class classification      Deep Learning with Python\n  Personal DS portfolio website\n Hugo, Github pages, Git    Documentation\n Readthedocs, sphinx, .rtf\u0026rsquo;s file    Reporting\n Readme    Projects  Advanced house price prediction image classification using dogs-cats, kaggle intel datasets  3rd phase   Applied Time-series analysis book\n  An Intro to Statistic learning book\n  Elements of Statistic learning\n  NLP\n Spacy NLTK book    Excel\n  Probability \u0026amp; Statistics\n  Boosting Algorithms\n  ","permalink":"https://chunyip135.github.io/home/publications/my_ds_pathway/","tags":["Data Science","Roadmap"],"title":"My Personal Data Science Roadmap"},{"categories":null,"contents":"Shields.io is a massive library of badges that can be inserted into project README\u0026rsquo;s or websites displaying various statuses (code coverage, health, version, etc). Support for docker was missing the current build health, and was a pretty trivial addition.\n","permalink":"https://chunyip135.github.io/home/projects/contributions/shields-docker/","tags":["Docker","Rest APIs","JavaScript","node.js","JSON"],"title":"Added Docker Build Status Badge to shields.io"},{"categories":null,"contents":"While adding Structured Data to a client\u0026rsquo;s website I found some example JSON that was invalid. Simple contribution to cleanup the user documentation providing syntactically valid JSON documents.\n","permalink":"https://chunyip135.github.io/home/projects/contributions/schema-org/","tags":["JSON"],"title":"Schema.org Structured Data documentation fixes"},{"categories":null,"contents":"BOSH (Bosh Outer SHell) \u0026ldquo;\u0026hellip;is an open source tool for release engineering, deployment, lifecycle management, and monitoring of distributed systems.\u0026rdquo; And it\u0026rsquo;s amazingly powerful. This examples uses BOSH to provision an Alassian vendor app running on JDK along with the support Postgres database and agents to support it. The releases manages the health of services and will automatically provision, start/stop processes across the various services.\n","permalink":"https://chunyip135.github.io/home/projects/creations/bosh-agents/","tags":["DevOps","BOSH","Java","Atlassian Ecosystem","monit","python","xml/xslt","bash/shell","REST APIs"],"title":"BOSH release for Bamboo \u0026 Remote Agents"},{"categories":null,"contents":"Descriptions This is a project that uses Convolutional Neural Nets to classified images between classes of forest, glacier, sea, street and buildings. The libraries that are used are such as tensorflow, keras, matplotlib and numpy. The datasets is open-source and can be found on Kaggle.\n visualize images visualize covnets model deploy to web  import tensorflow as tf from tensorflow import keras import cv2 import os from tensorflow.keras.preprocessing.image import ImageDataGenerator import matplotlib.pyplot as plt os.listdir(\u0026#34;../input/intel-image-classification/seg_train/seg_train\u0026#34;) ['glacier', 'sea', 'forest', 'street', 'mountain', 'buildings']  Import images using generator train_datagen = ImageDataGenerator(rescale = 1./255) test_datagen = ImageDataGenerator(rescale = 1./255) train_generator = train_datagen.flow_from_directory(\u0026#39;../input/intel-image-classification/seg_train/seg_train\u0026#39;, target_size = (150,150), batch_size = 128,class_mode=\u0026#39;sparse\u0026#39;) test_generator = test_datagen.flow_from_directory(\u0026#39;../input/intel-image-classification/seg_test/seg_test\u0026#39;, target_size = (150,150), batch_size = 128,class_mode=\u0026#39;sparse\u0026#39;) Found 14034 images belonging to 6 classes. Found 3000 images belonging to 6 classes.  Import images for EDA purposes def get_images(directory): Images = [] Labels = [] label = 0 for labels in os.listdir(directory): #Main Directory where each class label is present as folder name. if labels == \u0026#39;glacier\u0026#39;: #Folder contain Glacier Images get the \u0026#39;2\u0026#39; class label. label = 2 elif labels == \u0026#39;sea\u0026#39;: label = 4 elif labels == \u0026#39;buildings\u0026#39;: label = 0 elif labels == \u0026#39;forest\u0026#39;: label = 1 elif labels == \u0026#39;street\u0026#39;: label = 5 elif labels == \u0026#39;mountain\u0026#39;: label = 3 for image_file in os.listdir(directory+labels): #Extracting the file name of the image from Class Label folder image = cv2.imread(directory+labels+r\u0026#39;/\u0026#39;+image_file) #Reading the image (OpenCV) image = cv2.resize(image,(150,150)) #Resize the image, Some images are different sizes. (Resizing is very Important) Images.append(image) Labels.append(label) return Images, Labels def get_classlabel(class_code): labels = {2:\u0026#39;glacier\u0026#39;, 4:\u0026#39;sea\u0026#39;, 0:\u0026#39;buildings\u0026#39;, 1:\u0026#39;forest\u0026#39;, 5:\u0026#39;street\u0026#39;, 3:\u0026#39;mountain\u0026#39;} return labels[class_code] images, labels = get_images(\u0026#39;../input/intel-image-classification/seg_train/seg_train/\u0026#39;) #Extract the training images from the folders. images = np.array(images) #converting the list of images to numpy array. labels = np.array(labels) print(\u0026#39;Shape of images : \u0026#39;,images.shape) print(\u0026#39;Shape of labels : \u0026#39;,labels.shape) Shape of images : (14034, 150, 150, 3) Shape of labels : (14034,)  images[0] array([[[159, 142, 123], [158, 141, 122], [157, 140, 121], ..., [171, 155, 143], [172, 156, 144], [172, 156, 144]], [[154, 137, 118], [155, 138, 119], [157, 140, 121], ..., [172, 156, 144], [172, 156, 144], [172, 156, 144]], [[153, 136, 117], [154, 137, 118], [156, 139, 120], ..., [172, 156, 144], [172, 156, 144], [172, 156, 144]], ..., [[ 4, 15, 13], [ 6, 17, 15], [ 6, 17, 15], ..., [ 4, 14, 8], [ 36, 43, 38], [ 17, 24, 19]], [[ 4, 15, 13], [ 3, 14, 12], [ 3, 14, 12], ..., [ 26, 40, 36], [ 26, 37, 34], [ 48, 57, 54]], [[ 0, 6, 4], [ 0, 4, 2], [ 0, 9, 7], ..., [ 8, 23, 19], [ 19, 30, 27], [ 14, 25, 22]]], dtype=uint8)  num_classes = len(np.unique(labels)) Visualize the images _, ax = plt.subplots(5,5, figsize = (15,10)) for i in range(5): for j in range(5): r = np.random.randint(len(images)) # randomly select 25 images ax[i,j].imshow(images[r]) ax[i,j].set_title(get_classlabel(labels[r])) ax[i,j].axis(\u0026#39;off\u0026#39;) plt.savefig(\u0026#39;cnn.png\u0026#39;) _, ax = plt.subplots() pd.Series(labels).map({2:\u0026#39;glacier\u0026#39;, 4:\u0026#39;sea\u0026#39;, 0:\u0026#39;buildings\u0026#39;, 1:\u0026#39;forest\u0026#39;, 5:\u0026#39;street\u0026#39;, 3:\u0026#39;mountain\u0026#39;}).value_counts().sort_index().plot(kind = \u0026#39;bar\u0026#39;,ax=ax) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f1b5a7594d0\u0026gt;  Model class myCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if(logs.get(\u0026#39;acc\u0026#39;)\u0026gt;0.85): print(\u0026#34;\\nReached 85% accuracy so cancelling training!\u0026#34;) self.model.stop_training = True callbacks = myCallback() model = keras.models.Sequential([ keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;, input_shape=(150,150,3)), keras.layers.MaxPooling2D(2, 2), keras.layers.Conv2D(64, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2, 2), keras.layers.Conv2D(32, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2, 2), keras.layers.Conv2D(16, (3,3), activation=\u0026#39;relu\u0026#39;), keras.layers.MaxPooling2D(2, 2), keras.layers.Flatten(), keras.layers.Dense(128, activation=\u0026#39;relu\u0026#39;), keras.layers.Dense(num_classes, activation=\u0026#39;softmax\u0026#39;) ]) model.summary() Model: \u0026quot;sequential\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 148, 148, 64) 1792 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 74, 74, 64) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 72, 72, 64) 36928 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 34, 34, 32) 18464 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 17, 17, 32) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 15, 15, 16) 4624 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 7, 7, 16) 0 _________________________________________________________________ flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 128) 100480 _________________________________________________________________ dense_1 (Dense) (None, 6) 774 ================================================================= Total params: 163,062 Trainable params: 163,062 Non-trainable params: 0 _________________________________________________________________  model.compile(loss = \u0026#39;sparse_categorical_crossentropy\u0026#39;, optimizer = \u0026#39;Adam\u0026#39;, metrics = [\u0026#39;acc\u0026#39;]) history = model.fit(train_generator,validation_data = test_generator, epochs = 20,callbacks = [callbacks], batch_size = 128) Epoch 1/20 110/110 [==============================] - 35s 315ms/step - loss: 1.1974 - acc: 0.5186 - val_loss: 1.0111 - val_acc: 0.6160 Epoch 2/20 110/110 [==============================] - 30s 272ms/step - loss: 0.8693 - acc: 0.6699 - val_loss: 0.8058 - val_acc: 0.6900 Epoch 3/20 110/110 [==============================] - 29s 265ms/step - loss: 0.7290 - acc: 0.7280 - val_loss: 0.7573 - val_acc: 0.7153 Epoch 4/20 110/110 [==============================] - 30s 275ms/step - loss: 0.6485 - acc: 0.7634 - val_loss: 0.6422 - val_acc: 0.7553 Epoch 5/20 110/110 [==============================] - 28s 257ms/step - loss: 0.5823 - acc: 0.7911 - val_loss: 0.5794 - val_acc: 0.7937 Epoch 6/20 110/110 [==============================] - 29s 268ms/step - loss: 0.5351 - acc: 0.8071 - val_loss: 0.5537 - val_acc: 0.8057 Epoch 7/20 110/110 [==============================] - 28s 259ms/step - loss: 0.5152 - acc: 0.8100 - val_loss: 0.5237 - val_acc: 0.8127 Epoch 8/20 110/110 [==============================] - 30s 269ms/step - loss: 0.4737 - acc: 0.8287 - val_loss: 0.4953 - val_acc: 0.8217 Epoch 9/20 110/110 [==============================] - 29s 267ms/step - loss: 0.4361 - acc: 0.8421 - val_loss: 0.5356 - val_acc: 0.8133 Epoch 10/20 110/110 [==============================] - 30s 274ms/step - loss: 0.4354 - acc: 0.8404 - val_loss: 0.6009 - val_acc: 0.7787 Epoch 11/20 110/110 [==============================] - 29s 265ms/step - loss: 0.4193 - acc: 0.8487 - val_loss: 0.5276 - val_acc: 0.8107 Epoch 12/20 110/110 [==============================] - ETA: 0s - loss: 0.3948 - acc: 0.8573 Reached 85% accuracy so cancelling training! 110/110 [==============================] - 30s 276ms/step - loss: 0.3948 - acc: 0.8573 - val_loss: 0.5287 - val_acc: 0.8037  score = pd.DataFrame(history.history) _, ax = plt.subplots(1,2,figsize = (10,6)) score[[\u0026#39;acc\u0026#39;,\u0026#39;val_acc\u0026#39;]].plot(ax=ax[0]) score[[\u0026#39;loss\u0026#39;,\u0026#39;val_loss\u0026#39;]].plot(ax=ax[1]) ax[0].set(xlabel = \u0026#39;epochs\u0026#39;,ylabel = \u0026#39;accuracy\u0026#39;,title = \u0026#39;acc \u0026amp; valid\\\u0026#39;s acc\u0026#39;) ax[1].set(xlabel = \u0026#39;epochs\u0026#39;,ylabel = \u0026#39;loss\u0026#39;,title = \u0026#39;loss \u0026amp; valid\\\u0026#39;s loss\u0026#39;) [Text(0, 0.5, 'loss'), Text(0.5, 0, 'epochs'), Text(0.5, 1.0, \u0026quot;loss \u0026amp; valid's loss\u0026quot;)]  Import images from pred directory images_test, labels_test = get_images(\u0026#39;../input/intel-image-classification/seg_test/seg_test/\u0026#39;) #Extract the training images from the folders. images_test = np.array(images_test) #converting the list of images to numpy array. labels_test = np.array(labels_test) def random_predict_with_labels(): r = np.random.randint(len(images_test)) result = model.predict(np.expand_dims(images_test[r],axis = 0)).reshape(6) print(\u0026#39;Predicted : \u0026#39;,get_classlabel(np.argmax(result))) if get_classlabel(np.argmax(result)) == get_classlabel(labels_test[r]): print(\u0026#39;Prediction is accurate!\u0026#39;) else: print(\u0026#39;Prediction is wrong! \\nCorrect prediction is {}\u0026#39;.format(get_classlabel(labels_test[r]))) fig,ax = plt.subplots(1,2,figsize = (10,6)) ax[0].imshow(images_test[r]) ax[0].axis(\u0026#39;off\u0026#39;) ax[1].bar(np.arange(6), result) plt.xticks(np.arange(6),[\u0026#39;buildings\u0026#39;,\u0026#39;forest\u0026#39;,\u0026#39;glacier\u0026#39;,\u0026#39;mountain\u0026#39;,\u0026#39;sea\u0026#39;,\u0026#39;street\u0026#39;], rotation = 45) plt.title(\u0026#39;Probability of each class\u0026#39;) random_predict_with_labels() Predicted : glacier Prediction is accurate!  Applying the model to predict on Pred\u0026rsquo;s sets def pred_image(directory): Images = [] for image_file in os.listdir(directory): #Extracting the file name of the image from Class Label folder image = cv2.imread(directory+r\u0026#39;/\u0026#39;+image_file) #Reading the image (OpenCV) image = cv2.resize(image,(150,150)) #Resize the image, Some images are different sizes. (Resizing is very Important) Images.append(image) return Images images_pred = pred_image(\u0026#34;../input/intel-image-classification/seg_pred/seg_pred\u0026#34;) images_pred = np.array(images_pred) print(images_pred.shape) images_pred = images_pred / 255.0 # normalize the pixels to value between [0,1] def random_predict(): r = np.random.randint(len(images_pred)) result = model.predict(np.expand_dims(images_pred[r],axis = 0)).reshape(6) print(get_classlabel(np.argmax(result))) fig,ax = plt.subplots(1,2,figsize = (10,6)) ax[0].imshow(images_pred[r]) ax[0].axis(\u0026#39;off\u0026#39;) ax[1].bar(np.arange(6), result) plt.xticks(np.arange(6),[\u0026#39;buildings\u0026#39;,\u0026#39;forest\u0026#39;,\u0026#39;glacier\u0026#39;,\u0026#39;mountain\u0026#39;,\u0026#39;sea\u0026#39;,\u0026#39;street\u0026#39;], rotation = 45) plt.title(\u0026#39;Probability of each class\u0026#39;) random_predict() sea  ","permalink":"https://chunyip135.github.io/home/projects/creations/cnn-notebook_commited/","tags":["Python","keras","deep learning","tensorflow","CNN"],"title":"Project 1 : Image Classification"},{"categories":null,"contents":"Intro Doesn\u0026rsquo;t matter whether it\u0026rsquo;s a CakePHP app for a client, your own personal CMS, or any other web based application. If your passing around passwords or other sensitive info you should really implement SSL. SSL provides 2 main perks to your visitors.\n First it encrypts all communication that flies across the web. This prevents curious or devious billies from getting your secrets. Secondly it ensures to the user that your server is in fact who it claims, and not a nasty \u0026lsquo;man in the middle\u0026rdquo; attack. Finally it gives your site that touch of class\u0026hellip;. which of course a classy person like yourself relies on.  Once you implement SSL certificates on your server you\u0026rsquo;ll want to require secure connections using Apache\u0026rsquo;s rewrite module. Now I won\u0026rsquo;t dwell on the creation and signing of certificates, its already well documented. If your just starting out though,heres a few links I recommend;\n Creating self-signed certificates (free, but should only be used internally or for testing, users will; see an \u0026lsquo;Untrusted\u0026rdquo; warning) Requesting a CA Signed certificate (not free, but the final certificate is trusted and seamless for users)  The second link uses the schools internal CA, you will need to pay a public CA like Entrust or Verisign. All of this information is aimed at \u0026lsquo;nix or solaris servers running apache. Why? cause a production windows server is laughable :-p\nNow that you have a certificate, whats next? So there you are you have a shiny new Certificate and Server key, how do you force visitors to your apache driven site to use the SSL? You copied the certificates into the appropite locations right? And you have made the needed changes in httpd.conf right? So now when you view https://example.com you see a \u0026lsquo;trusted\u0026rsquo; warning or your site right? If No to any of these than this article does a pretty good job of outlining those steps.\nThe SSL Works, How do I force connections to use it? First you need to decide if you want to force every page on your site to use SSL, or only a particular sub-domain, or maybe just your admin directory. Since the overhead is minimal there is no harm is forcing the entire domain to leverage SSL, but if it is a self-signed certificate for your personal use than you\u0026rsquo;ll most certainly want to restrict its use to your own areas. This prevents users from seeing that nasty warning \u0026ldquo;This server is not trusted\u0026rdquo; You\u0026rsquo;ll know if your using SSL because the url prefix changes from http to https (s for secure).\nForcing entire domain to use SSL You want any visit, any where to use ssl. This probably the simplest solution. Create or append to your htaccess file in the top directory of your server. Some people use a port check (80 is typically http, while 443 is https) but if you have alernate configs or the user just adds :8080 to the end of the url this method is useless. Instead check whether the https environmental variable is set, if not then redirect.\nRewriteCond %{HTTPS} !=on RewriteRule ^(.*)$ https://%{SERVER_NAME}$1 \\[R,L\\] Forcing sub-domains to use SSL Maybe you only want mysecretarea.example.com to use SSL, that\u0026rsquo;s easy enough. Its the same premise as above, but you move the htaccess file into the directory that corresponds to the subdomain. Also change the second line like below;\nRewriteCond %{HTTPS} !=on RewriteRule ^(.*)$ https://mysecretarea.%{SERVER_NAME}$1 \\[R,L\\] Forcing a directory to use SSL This method cn get a little hairier if your using aliases or redirects on top of this one. You\u0026rsquo;ll need to consider what order the commands are read. The basic principle is like so. You want all visits to example.com/admin to use ssl. Create a htaccess file in the parent directory. Again will check for the https variable, but this time we also check for the sub-directory to be in the path.\nRewriteCond %{HTTPS} !=on RewriteRule ^/admin/(.*)$ https://%{SERVER_NAME}/admin/$1 \\[R,L\\] ","permalink":"https://chunyip135.github.io/home/blog/force-ssl/","tags":["apache","apache","redirect","rewrite","ssl","web development"],"title":"Forcing Visits to use SSL"},{"categories":null,"contents":"Multiple plugins used by thousands of teams that provide enhanced functionality of Atlassian’s core products (primarily JIRA and Bamboo) to enrich CI/CD capabilities, DevOps automation, or productivity. Functionality spans user interface, web services and persistence.\n","permalink":"https://chunyip135.github.io/home/projects/creations/marketplace/","tags":["Java","Spring","REST APIs","Javascript","Atlassian Developer Ecosystem","Bamboo","JIRA","Bitbucket","Confluence","DevOps"],"title":"Atlassian Marketplace Plugins"},{"categories":null,"contents":"Provides required dependencies and additional utilities to simplify and codify the process of building, testing and delivering Atlassian plugins all the way to the live marketplace.Executes integration/AUT level tests against all stated compatible versions for the productUploads generated artifact to Atlassian marketplaceProvides corresponding metadata indicating version, release notes, and compatibility","permalink":"https://chunyip135.github.io/home/projects/creations/docker-marketplace/","tags":["Docker","Maven","Java","Python","REST APIs","Bash/Shell"],"title":"Docker image for Bitbucket CI/CD Pipelines  \"shipit\""},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"https://chunyip135.github.io/home/search/","tags":null,"title":"Search Results"}]